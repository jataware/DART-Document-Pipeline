{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get started and connect to ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from jsonschema import validate\n",
    "from elasticsearch import Elasticsearch, RequestsHttpConnection\n",
    "from elasticsearch.helpers import scan\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tika import parser\n",
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "region = 'us-east-1'\n",
    "service = 'es'\n",
    "eshost = 'search-world-modelers-dev-gjvcliqvo44h4dgby7tn3psw74.us-east-1.es.amazonaws.com'\n",
    "\n",
    "session = boto3.Session(region_name=region, profile_name='wmuser')\n",
    "credentials = session.get_credentials()\n",
    "credentials = credentials.get_frozen_credentials()\n",
    "access_key = credentials.access_key\n",
    "secret_key = credentials.secret_key\n",
    "token = credentials.token\n",
    "\n",
    "aws_auth = AWS4Auth(\n",
    "    access_key,\n",
    "    secret_key,\n",
    "    region,\n",
    "    service,\n",
    "    session_token=token\n",
    ")\n",
    "\n",
    "es = Elasticsearch(\n",
    "    hosts = [{'host': eshost, 'port': 443}],\n",
    "    http_auth=aws_auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n",
    "\n",
    "index = 'wm-dev'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest MITRE Spreadsheet and obtain stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1287 total documents in the MITRE spreadsheet. There are the following counts per tab:\n",
      "\n",
      "{\n",
      "  \"Additional Six-Month Eval Docs\": 31,\n",
      "  \"Luma-Provided Ethiopia Docs\": 32,\n",
      "  \"November 2019 Ethiopia Docs\": 265,\n",
      "  \"November 2019 SSudan Docs\": 431,\n",
      "  \"Six-Month Evaluation Documents\": 52,\n",
      "  \"Twelve-Month Eval Docs\": 476\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "xls = pd.ExcelFile('/Users/brandon/Downloads/Six_Twelve-Month_November_2019%20Evaluation%20Documents%20-%20Updated%20-%2026%20June%202019.xlsx')\n",
    "\n",
    "ignore_sheets = ['ReadMe','Introduction','Copyright & Terms of Use']\n",
    "\n",
    "sheets = {}\n",
    "for sheet in xls.sheet_names:\n",
    "    if sheet not in ignore_sheets:\n",
    "        df = pd.read_excel(xls, sheet)\n",
    "        sheets[sheet] = df.shape[0]\n",
    "\n",
    "total = 0\n",
    "for k in sheets:\n",
    "    total += sheets[k]\n",
    "    \n",
    "print(f\"There are {total} total documents in the MITRE spreadsheet. There are the following counts per tab:\\n\")        \n",
    "print(json.dumps(sheets, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Garbage text extraction\n",
    "This document in ES clearly had failed text extraction, however when we open the document there is clearly text. What could be happening? Note: both extractors seem to fail.\n",
    "\n",
    "If we can't resolve this, we should at least characterize this issue by determining **how many documents are effected by \"silently\" failed text extraction?** (extractor succeeds but produces too little text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_path = 'https://reliefweb.int/sites/reliefweb.int/files/resources/South_Sudan_KeyMessages_Sept2017.pdf'  \n",
    "\n",
    "query = {\n",
    "        \"query\": {\n",
    "            \"match\" : {\n",
    "                \"source_url.keyword\" : url_path\n",
    "            }\n",
    "        }\n",
    "    }            \n",
    "            \n",
    "res = es.search(index=index, body=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'took': 0,\n",
       " 'timed_out': False,\n",
       " '_shards': {'total': 5, 'successful': 5, 'skipped': 0, 'failed': 0},\n",
       " 'hits': {'total': 1,\n",
       "  'max_score': 5.1279244,\n",
       "  'hits': [{'_index': 'wm-dev',\n",
       "    '_type': 'wm-document',\n",
       "    '_id': 'a55811aee1d1fa8dbe83fd5ee4045ae69f71b27cb6ec33ae7aa1ae0432181489',\n",
       "    '_score': 5.1279244,\n",
       "    '_source': {'stored_url': 'https://world-modelers.s3.amazonaws.com/documents/migration/tmp/DEV/SouthSudanKeyMessagesSept2017.pdf',\n",
       "     'file_name': 'SouthSudanKeyMessagesSept2017.pdf',\n",
       "     'file_type': '.pdf',\n",
       "     'modification_date': {'date': '2017-11-06T05:45:35Z'},\n",
       "     'creation_date': {'date': '2017-11-06T05:45:35Z'},\n",
       "     'source': {'author_name': 'Kerandi, Nicholas (FAOSS)'},\n",
       "     'category': 'Six-Month Evaluation Documents',\n",
       "     'extracted_text': {'pypdf2': '1\\n \\n \\n\\n \\n \\n\\n \\n\\n \\n\\n \\n5\\n \\n\\n \\n\\n \\n                                        \\n                  \\n \\n3\\n \\n \\n \\n2\\n \\n \\n3\\n \\n \\n4\\n \\n \\n5\\n \\n \\n',\n",
       "      'tika': '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1 \\n \\n\\n\\uf0b7 \\n\\n \\n\\n\\uf0b7 \\n\\n\\uf0b7 \\n\\n\\uf0b7 \\n5 \\n\\n\\uf0b7 \\n\\n\\uf0b7 \\n\\n                                                           \\n\\n3  \\n\\n \\n\\n\\n\\n2 \\n \\n\\n\\n\\n3 \\n \\n\\n\\n\\n4 \\n \\n\\n\\n\\n5 \\n \\n\\n\\n'},\n",
       "     'source_url': 'https://reliefweb.int/sites/reliefweb.int/files/resources/South_Sudan_KeyMessages_Sept2017.pdf'}}]}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download this document and try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/p37/lib/python3.7/site-packages/urllib3/connectionpool.py:851: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2195054"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################\n",
    "###### Define helper functions #####\n",
    "####################################\n",
    "def slugify(value):\n",
    "    return ''.join([c for c in value if c.isalpha() or c.isdigit() or c ==' ' or c == '.']).rstrip()\n",
    "\n",
    "def get_filename(cd, url, title):\n",
    "    if not cd:\n",
    "        return f\"{os.path.basename(url)[:225].strip()}{'.html' if '.' not in os.path.basename(url)[:225] else ''}\" or f'{title[:225]}.html'\n",
    "    fname = re.findall('filename=(.+)', cd)\n",
    "    if len(fname) == 0:\n",
    "        return f\"{os.path.basename(url)[:225].strip()}{'.html' if '.' not in os.path.basename(url)[:225] else ''}\" or f'{title[:225]}.html'\n",
    "    return f\"{fname[0].strip()}{'.html' if '.' not in fname[0] else ''}\"\n",
    "####################################\n",
    "####################################\n",
    "####################################\n",
    "\n",
    "\n",
    "# Download file\n",
    "doc_name = 'KEY IPC FINDINGS : SEPTEMBER 2017 â€“ MARCH 2018'\n",
    "r = requests.get(url_path, verify=False, stream=True, allow_redirects=True)\n",
    "r.raw.decode_content = True\n",
    "filename = f\"{slugify(get_filename(r.headers.get('content-disposition'), url_path, doc_name))}\"\n",
    "open(filename, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tika_data = parser.from_file('/Users/brandon/repos/Document-Schema/SouthSudanKeyMessagesSept2017.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1 \\n \\n\\n\\uf0b7 \\n\\n \\n\\n\\uf0b7 \\n\\n\\uf0b7 \\n\\n\\uf0b7 \\n5 \\n\\n\\uf0b7 \\n\\n\\uf0b7 \\n\\n                                                           \\n\\n3  \\n\\n \\n\\n\\n\\n2 \\n \\n\\n\\n\\n3 \\n \\n\\n\\n\\n4 \\n \\n\\n\\n\\n5 \\n \\n\\n\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tika_data['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved `file_type` detection\n",
    "Detecting the correct `file_type` is important since it allows us to determine the correct extractor (HTML vs. PDF).\n",
    "\n",
    "Right now we use a pretty naive approach which is brittle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 types of files in our index. These are:\n",
      ".pdf: 972\n",
      ".html: 124\n",
      ".pdfua1: 65\n",
      ".1547047485: 9\n",
      ".pdfplatformhootsuite: 3\n",
      ".php: 3\n",
      ": 2\n",
      ".1557326170: 1\n",
      ".htm: 1\n",
      ".htmbm08: 1\n"
     ]
    }
   ],
   "source": [
    "query = {\n",
    "    \"aggs\" : {\n",
    "        \"file_types\" : {\n",
    "            \"terms\" : { \"field\" : \"file_type.keyword\" } \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "file_types = es.search(index=index, body=query)['aggregations']['file_types']['buckets']\n",
    "\n",
    "print(f\"There are {len(file_types)} types of files in our index. These are:\")\n",
    "for f in file_types:\n",
    "    print(f\"{f['key']}: {f['doc_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that this is problematic because unless the `file_type` is either `.html` or `.pdf` we do not parse it.\n",
    "\n",
    "We can take a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_path = 'https://www.who.int/hac/crises/ssd/south-sudan-idsr-26march-1apr2018.pdf?ua=1'\n",
    "\n",
    "query = {\n",
    "        \"query\": {\n",
    "            \"match\" : {\n",
    "                \"file_type.keyword\" : '.pdfua1'\n",
    "            }\n",
    "        }\n",
    "    }            \n",
    "            \n",
    "res = es.search(index=index, body=query)['hits']['hits']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `extracted_text` is empty here, since no extraction was performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'wm-dev',\n",
       " '_type': 'wm-document',\n",
       " '_id': 'e8716caf92dfcaacf0d979e402d6767804e9de9685dd0c3b46323c09d2c7b14a',\n",
       " '_score': 2.9306998,\n",
       " '_source': {'stored_url': 'https://world-modelers.s3.amazonaws.com/documents/migration/tmp/DEV/southsudanidsrannex8October2017.pdfua1',\n",
       "  'file_name': 'southsudanidsrannex8October2017.pdfua1',\n",
       "  'file_type': '.pdfua1',\n",
       "  'category': 'November 2019 SSudan Docs',\n",
       "  'extracted_text': {},\n",
       "  'source_url': 'https://www.who.int/hac/crises/ssd/south-sudan-idsr-annex-8October2017.pdf?ua=1'}}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract a better `file_type` with a regex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_url = res[0]['_source']['source_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.pdf'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = re.compile(r\".*(\\.pdf|\\.html|\\.docx|\\.doc|\\.ppt|\\.pptx).*\")\n",
    "result = pattern.match(source_url)\n",
    "result.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this doesn't explain all the failed extractions. We can check the types of extractions and their counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"query\": {\n",
    "        \"exists\": {\n",
    "            \"field\": \"extracted_text.bs4\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "bs4_count = es.count(index=index, body=query)['count']\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"exists\": {\n",
    "            \"field\": \"extracted_text.tika\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "tika_count = es.count(index=index, body=query)['count']\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"exists\": {\n",
    "            \"field\": \"extracted_text.pypdf2\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "pypdf2_count = es.count(index=index, body=query)['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We extracted the following counts per extractor\n",
      "BS4: 56\n",
      "Tika: 971\n",
      "PyPDF2: 955\n"
     ]
    }
   ],
   "source": [
    "print(\"We extracted the following counts per extractor\\n\"\\\n",
    "      f\"BS4: {bs4_count}\\n\"\\\n",
    "      f\"Tika: {tika_count}\\n\"\\\n",
    "      f\"PyPDF2: {pypdf2_count}\"\\\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there were 124 `.html` files what happened? We can check for documents with the `.html` `file_type` but that are missing `bs4` extractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"query\": {\n",
    "        \"query_string\" : {\n",
    "            \"query\" : \"(file_type: .html) AND (NOT _exists_:extracted_text.bs4)\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "res = es.search(index=index, body=query)['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'wm-dev',\n",
       " '_type': 'wm-document',\n",
       " '_id': 'e4cec19518356b6f5c107c192a779f1205622fef2d4315cf08caf25dc5552090',\n",
       " '_score': 3.8596,\n",
       " '_source': {'stored_url': 'https://world-modelers.s3.amazonaws.com/documents/migration/tmp/DEV/52304.html',\n",
       "  'file_name': '52304.html',\n",
       "  'file_type': '.html',\n",
       "  'category': 'November 2019 SSudan Docs',\n",
       "  'extracted_text': {},\n",
       "  'source_url': 'https://data2.unhcr.org/en/documents/download/52304'}}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note if we actually take a look at the `source_url` we will see that it is really a **`PDF`**, _not_ an **`HTML`** file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_path = res[0]['_source']['source_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings [connectionpool.py:851]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "411942"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download file\n",
    "doc_name = 'South Sudan Situation Regional Situation'\n",
    "r = requests.get(url_path, verify=False, stream=True, allow_redirects=True)\n",
    "r.raw.decode_content = True\n",
    "filename = f\"{slugify(get_filename(r.headers.get('content-disposition'), url_path, doc_name))}\"\n",
    "open(filename, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Python's (magic library)[https://github.com/ahupp/python-magic] we can deal with this. \n",
    "\n",
    "```\n",
    "pip install python-magic-bin==0.4.14\n",
    "```\n",
    "\n",
    "Note that even though the filename has `.html` it still correctly detects it as a PDF.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'52304.html'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application/pdf'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import magic\n",
    "mime = magic.Magic(mime=True)\n",
    "mime.from_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appears to be a pretty common problem for documents from `November 2019 SSudan Docs` tab of the spreadsheet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
